{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A89RJ-9qvMmv"
      },
      "source": [
        "# Chapter 2: Working with Text Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysnlf45_zG0p",
        "outputId": "db921751-958f-420f-b38b-3e24fc2d7559"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n",
            "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/1.2 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tx817NCeu_SS",
        "outputId": "b815b5f6-74c9-413c-c0de-fb43c1ac2b06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch version: 2.5.1+cu121\n",
            "tiktoken version: 0.8.0\n"
          ]
        }
      ],
      "source": [
        "# packages that are being used in this notebook\n",
        "from importlib.metadata import version\n",
        "\n",
        "print('torch version:', version('torch'))\n",
        "print('tiktoken version:', version('tiktoken'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3d5s8sh1vqtT"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "import os\n",
        "import urllib.request\n",
        "\n",
        "if not os.path.exists('the-verdict.txt'):\n",
        "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
        "    \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
        "    \"the-verdict.txt\")\n",
        "\n",
        "    file_path = 'the-verdict.txt'\n",
        "    urllib.request.urlretrieve(url, file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kqCV_WUwM7b",
        "outputId": "9f559b7b-554b-48a7-8ec0-96d62669d814"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of character: 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ],
      "source": [
        "with open('the-verdict.txt', 'r', encoding='utf-8') as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "print('Total number of character:', len(raw_text))\n",
        "print(raw_text[:99])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnjHNCC0w6Gq",
        "outputId": "0210928d-6e37-4235-bc81-3d9a710f2f21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "text = 'Hello, world. This, is a test.'\n",
        "result = re.split(r'(\\s)', text)\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dtcXoLdxfR7",
        "outputId": "8db13428-caa3-4e99-e2ab-1b5eca1da5c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
          ]
        }
      ],
      "source": [
        "result = re.split(r'([,.]|\\s)', text)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hnUsTcox2-s",
        "outputId": "577b171c-a900-4661-c3df-d7d16f641771"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
          ]
        }
      ],
      "source": [
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yabg6VY9yLV4",
        "outputId": "a0ce1021-61e5-4982-ca9a-97fb044fc7f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
          ]
        }
      ],
      "source": [
        "text = \"Hello, world. Is this-- a test?\"\n",
        "\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "result = [item.strip() for item in result if item.strip()]\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U82sltnZy26A",
        "outputId": "5033d5d8-b5c5-4d9d-9bd8-aefd69f054b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
          ]
        }
      ],
      "source": [
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(preprocessed[:30])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8cpxrbIzUQ4",
        "outputId": "67185509-48af-48c6-f179-25a5d6c57b9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4690\n"
          ]
        }
      ],
      "source": [
        "print(len(preprocessed))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exQkJHSOzeZk",
        "outputId": "03690e09-f050-4322-8ddd-af04fecd0b25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1130\n"
          ]
        }
      ],
      "source": [
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QTmg4rDz-T7"
      },
      "outputs": [],
      "source": [
        "vocab = {token:integer for integer, token in enumerate(all_words)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgxd5Gpi0IE4",
        "outputId": "85512c08-6354-4fe6-8b25-9a422b232c0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('--', 6)\n",
            "('.', 7)\n",
            "(':', 8)\n",
            "(';', 9)\n",
            "('?', 10)\n",
            "('A', 11)\n",
            "('Ah', 12)\n",
            "('Among', 13)\n",
            "('And', 14)\n",
            "('Are', 15)\n",
            "('Arrt', 16)\n",
            "('As', 17)\n",
            "('At', 18)\n",
            "('Be', 19)\n",
            "('Begin', 20)\n",
            "('Burlington', 21)\n",
            "('But', 22)\n",
            "('By', 23)\n",
            "('Carlo', 24)\n",
            "('Chicago', 25)\n",
            "('Claude', 26)\n",
            "('Come', 27)\n",
            "('Croft', 28)\n",
            "('Destroyed', 29)\n",
            "('Devonshire', 30)\n",
            "('Don', 31)\n",
            "('Dubarry', 32)\n",
            "('Emperors', 33)\n",
            "('Florence', 34)\n",
            "('For', 35)\n",
            "('Gallery', 36)\n",
            "('Gideon', 37)\n",
            "('Gisburn', 38)\n",
            "('Gisburns', 39)\n",
            "('Grafton', 40)\n",
            "('Greek', 41)\n",
            "('Grindle', 42)\n",
            "('Grindles', 43)\n",
            "('HAD', 44)\n",
            "('Had', 45)\n",
            "('Hang', 46)\n",
            "('Has', 47)\n",
            "('He', 48)\n",
            "('Her', 49)\n",
            "('Hermia', 50)\n"
          ]
        }
      ],
      "source": [
        "for i, item in enumerate(vocab.items()):\n",
        "    print(item)\n",
        "    if i >= 50:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuBIfCde0Qw3"
      },
      "outputs": [],
      "source": [
        "class SimpleTokenizerV1:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [\n",
        "            item.strip() for item in preprocessed if item.strip()\n",
        "        ]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEyRR20p2C_1",
        "outputId": "018a51de-4b64-473f-b957-0ed0407d9731"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
          ]
        }
      ],
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "text = \"\"\"\"It's the last he painted, you know,\"\n",
        "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "e5WYEnD_2Vq1",
        "outputId": "485976e3-e674-44b2-f4b6-b6ad880827ce"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEzQwGOI2c84",
        "outputId": "b1b1a822-3745-4588-cf83-7b2fc7b07a07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Key Error: 'Hello'\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    text = \"Hello, do you like tea?\"\n",
        "    tokenizer.encode(text)\n",
        "except KeyError as err:\n",
        "    print(\"Key Error:\", err)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nWVeFaz3LFP"
      },
      "outputs": [],
      "source": [
        "# adding special context tokens\n",
        "\n",
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "\n",
        "vocab =  {token:integer for integer, token in enumerate(all_tokens)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmKbdju24SC5",
        "outputId": "35d33283-e7a3-476b-927a-19692a0f9fe1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1132"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vocab.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgpgCxOP4Uyg",
        "outputId": "4b54293e-c555-4e7d-949a-f722f7899f7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('younger', 1127)\n",
            "('your', 1128)\n",
            "('yourself', 1129)\n",
            "('<|endoftext|>', 1130)\n",
            "('<|unk|>', 1131)\n"
          ]
        }
      ],
      "source": [
        "for item in list(vocab.items())[-5:]:\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ag3AzBgM4e1Z"
      },
      "outputs": [],
      "source": [
        "class SimpleTokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        preprocessed = [\n",
        "            item if item in self.str_to_int\n",
        "            else \"<|unk|>\" for item in preprocessed\n",
        "        ]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5TxPYg_5bd5",
        "outputId": "b500110b-ecf9-4040-fdf3-f9a82e1d7ccd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, do you like tea? <|endoftext|> The the sunlit terraces of the palace.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "\n",
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"The the sunlit terraces of the palace.\"\n",
        "\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oZPhSJM5xOd",
        "outputId": "baa779cb-42eb-421c-98ad-c03b53750aed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1131, 5, 355, 1126, 628, 975, 10, 1130, 93, 988, 956, 984, 722, 988, 1131, 7]\n"
          ]
        }
      ],
      "source": [
        "encoded_text = tokenizer.encode(text)\n",
        "print(encoded_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kI8I4X_L56vC",
        "outputId": "e02b2e3c-c30d-4c13-ef4a-08f6803e921f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<|unk|>, do you like tea? <|endoftext|> The the sunlit terraces of the <|unk|>.'"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(encoded_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5MEqGkQ59Dk"
      },
      "outputs": [],
      "source": [
        "# Byte-pair encoding\n",
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding('gpt2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-3aNAGu7Tm9",
        "outputId": "b20dbef7-f19a-4e18-f857-bc160510c402"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
          ]
        }
      ],
      "source": [
        "text = (\n",
        "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
        "    \"of someunknownPlace.\"\n",
        ")\n",
        "integers = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "print(integers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZyHbY7u7m2c",
        "outputId": "f409adaf-14b0-4c0b-89db-0c3c46df8327"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
          ]
        }
      ],
      "source": [
        "strings = tokenizer.decode(integers)\n",
        "print(strings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNlhC-oh7srL",
        "outputId": "b9d08e42-aee4-4da3-877d-362411096ec2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5145\n"
          ]
        }
      ],
      "source": [
        "# Data samling\n",
        "with open('the-verdict.txt', 'r', encoding='utf-8') as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "enc_text = tokenizer.encode(raw_text)\n",
        "print(len(enc_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWydURbA9qVp"
      },
      "outputs": [],
      "source": [
        "enc_sample = enc_text[50:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLmoYQ6U9-jl",
        "outputId": "687006aa-6004-40c8-e7ee-80d8c1fee72d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x: [290, 4920, 2241, 287]\n",
            "y: [4920, 2241, 287, 257]\n"
          ]
        }
      ],
      "source": [
        "context_size = 4\n",
        "\n",
        "x = enc_sample[:context_size]\n",
        "y = enc_sample[1:context_size+1]\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y: {y}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VrVtP7d-QPV",
        "outputId": "0c486ca7-819d-43c4-ec96-342feb817e4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[290] ---> 4920\n",
            "[290, 4920] ---> 2241\n",
            "[290, 4920, 2241] ---> 287\n",
            "[290, 4920, 2241, 287] ---> 257\n"
          ]
        }
      ],
      "source": [
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "    print(context, '--->', desired)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMPLzmjk-sOV",
        "outputId": "7aae784d-5810-42ec-e371-6e96f3b8da9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " and --->  established\n",
            " and established --->  himself\n",
            " and established himself --->  in\n",
            " and established himself in --->  a\n"
          ]
        }
      ],
      "source": [
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "    print(tokenizer.decode(context), '--->', tokenizer.decode([desired]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LlG2gQ2a--Eu",
        "outputId": "d93a3106-8e31-49bb-b5ca-5477cefcc8bf"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.5.1+cu121'"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gt3GwmJrAD75"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i+max_length]\n",
        "            target_chunk = token_ids[i+1:i+max_length+1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yAE_IrFBJcy"
      },
      "outputs": [],
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding('gpt2')\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suh7SAQIBw6l",
        "outputId": "7a9dbf8b-b19e-4f6b-84eb-edbc1a614209"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
          ]
        }
      ],
      "source": [
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ouUQbY0CWTF",
        "outputId": "9ccce6e2-1e75-461f-c2a0-9b182bdc737b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inputs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "Targets:\n",
            " tensor([[  367,  2885,  1464,  1807],\n",
            "        [ 3619,   402,   271, 10899],\n",
            "        [ 2138,   257,  7026, 15632],\n",
            "        [  438,  2016,   257,   922],\n",
            "        [ 5891,  1576,   438,   568],\n",
            "        [  340,   373,   645,  1049],\n",
            "        [ 5975,   284,   502,   284],\n",
            "        [ 3285,   326,    11,   287]])\n"
          ]
        }
      ],
      "source": [
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=8, max_length=4, stride=4,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"Targets:\\n\", targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuQk9d6gzno2",
        "outputId": "0991d2b5-eef4-4826-8485-49b6e81f336c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.3374, -0.1778, -0.1690],\n",
            "        [ 0.9178,  1.5810,  1.3010],\n",
            "        [ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-1.1589,  0.3255, -0.6315],\n",
            "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "input_ids = torch.tensor([2, 3, 5, 1])\n",
        "vocab_size = 6\n",
        "output_dim = 3\n",
        "\n",
        "torch.manual_seed(123)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "print(embedding_layer.weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdPXUgTv0UH5",
        "outputId": "fb69d30f-c1dd-405d-9700-1a12509b24f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(embedding_layer(torch.tensor([3])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFQtZItV0mGk",
        "outputId": "721cc4bd-dd7c-46c8-a233-c7ce263cb06f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-2.8400, -0.7849, -1.4096],\n",
            "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(embedding_layer(input_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldYAdpBp01e5"
      },
      "outputs": [],
      "source": [
        "# Encoding word positions\n",
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMYFy48i3PFl",
        "outputId": "91fb5660-3f2f-4400-ffce-ac0fac86f202"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token IDs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Inputs shape:\n",
            " torch.Size([8, 4])\n"
          ]
        }
      ],
      "source": [
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=8, max_length=max_length,\n",
        "    stride=max_length, shuffle=False\n",
        ")\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Token IDs:\\n\", inputs)\n",
        "print(\"\\nInputs shape:\\n\", inputs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iqMyI5z3uRV",
        "outputId": "8071932e-b95b-4b58-a2ce-e32ab6ae7b54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ],
      "source": [
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(token_embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFnim5k44Eup",
        "outputId": "8200633a-e1de-4a65-b222-0bdcc5fda051"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ],
      "source": [
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
        "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
        "print(pos_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlbBERBA5fo4"
      },
      "source": [
        "# Appendix A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDhj9HVV4a31",
        "outputId": "8e9b396b-c265-4b69-9d3f-a6d105f3e7c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor([-0.0898]),)\n",
            "(tensor([-0.0817]),)\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.autograd import grad\n",
        "\n",
        "y = torch.tensor([1.0])\n",
        "x1 = torch.tensor([1.1])\n",
        "w1 = torch.tensor([2.2], requires_grad=True)\n",
        "b = torch.tensor([0.0], requires_grad=True)\n",
        "\n",
        "z = x1 * w1 + b\n",
        "a = torch.sigmoid(z)\n",
        "\n",
        "loss = F.binary_cross_entropy(a, y)\n",
        "\n",
        "grad_L_w1 = grad(loss, w1, retain_graph=True)\n",
        "grad_L_b = grad(loss, b, retain_graph=True)\n",
        "\n",
        "print(grad_L_w1)\n",
        "print(grad_L_b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2GVOxOb_pf2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class NeuralNetwork(torch.nn.Module):\n",
        "    def __init__(self, num_inputs, num_outputs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = torch.nn.Sequential(\n",
        "            # 1st hidden layer\n",
        "            torch.nn.Linear(num_inputs, 30),\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            # 2nd hidden layer\n",
        "            torch.nn.Linear(30, 20),\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            # output layer\n",
        "            torch.nn.Linear(20, num_outputs)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.layers(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "model = NeuralNetwork(50, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aO6dtiE5X8Mz",
        "outputId": "bf711073-c0fe-40c6-b983-495e0b0cc89b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NeuralNetwork(\n",
            "  (layers): Sequential(\n",
            "    (0): Linear(in_features=50, out_features=30, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=30, out_features=20, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=20, out_features=3, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocMvx45ZYGKY",
        "outputId": "97f901f9-80dd-4e93-aac6-d9c90f84f8e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logits1=tensor([-0.1404, -0.1096,  0.0126], grad_fn=<ViewBackward0>)\n",
            "probs=tensor([0.3128, 0.3226, 0.3645], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "x1 = torch.randn(50)\n",
        "logits1 = model(x1)\n",
        "print(f'{logits1=}')\n",
        "probs = torch.nn.Softmax(dim=0)(logits1) # torch.softmax(logits, dim=1)\n",
        "print(f'{probs=}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6t_ysxOYNEQ",
        "outputId": "d7f9fecd-5c22-4a23-db93-276911e2851e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num_params=2213\n"
          ]
        }
      ],
      "source": [
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"{num_params=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otO3xDapZNVW",
        "outputId": "512a0b1e-e16f-4015-9030-afd5e444c43d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.0590, -0.1167, -0.1109,  ...,  0.0418, -0.0664,  0.1084],\n",
            "        [-0.1167,  0.0489, -0.0160,  ...,  0.0291, -0.0536,  0.0133],\n",
            "        [ 0.0170, -0.1377,  0.0527,  ...,  0.0970,  0.0044,  0.0141],\n",
            "        ...,\n",
            "        [ 0.0349,  0.0673, -0.0366,  ..., -0.1031, -0.1050, -0.0131],\n",
            "        [ 0.1008, -0.0266,  0.0058,  ...,  0.0517,  0.0077, -0.0105],\n",
            "        [ 0.1203,  0.1377, -0.0354,  ..., -0.0970,  0.0474,  0.1329]],\n",
            "       requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "print(model.layers[0].weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZO6Evr0Zuxn",
        "outputId": "b037f0ba-39de-48c1-9724-d44480f419d3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([30, 50])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.layers[0].weight.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYXQ1wgNZ13t",
        "outputId": "30e498ee-f866-4862-ffa5-007a08aaa549"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([ 0.1323, -0.0839, -0.1320,  0.0486,  0.1137,  0.0720,  0.0674,  0.0393,\n",
              "         0.0540,  0.0638,  0.0925, -0.1187,  0.0302,  0.0269,  0.1236, -0.1113,\n",
              "         0.1176, -0.1412,  0.1154, -0.0304,  0.1120, -0.0291,  0.0223, -0.0597,\n",
              "        -0.1341, -0.1011,  0.0268, -0.0505, -0.0747,  0.0933],\n",
              "       requires_grad=True)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.layers[0].bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLKN9PQ2Z6LR",
        "outputId": "8e75347f-26ff-4553-8e17-8a8e6e983a4d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([30])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.layers[0].bias.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6TVcSLkZ87W",
        "outputId": "f93ffe54-d7fa-46f3-dab6-c68ac2b7a9d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.2955, 0.3358, 0.3687]])\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    X = torch.randn((1, 50))\n",
        "    out = model(X)\n",
        "    out = torch.softmax(out, dim=1)\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JQvnz3wa_6v"
      },
      "outputs": [],
      "source": [
        "X_train = torch.tensor([\n",
        "    [-1.2, 3.1],\n",
        "    [-0.9, 2.9],\n",
        "    [-0.5, 2.6],\n",
        "    [2.3, -1.1],\n",
        "    [2.7, -1.5]\n",
        "])\n",
        "y_train = torch.tensor([0, 0, 0, 1, 1])\n",
        "X_test = torch.tensor([\n",
        "    [-0.8, 2.8],\n",
        "    [2.6, -1.6],\n",
        "])\n",
        "y_test = torch.tensor([0, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ubJCTvNb24p"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class ToyDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.features = X\n",
        "        self.labels = y\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        one_x = self.features[index]\n",
        "        one_y = self.labels[index]\n",
        "        return one_x, one_y\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.labels.shape[0]\n",
        "\n",
        "train_ds = ToyDataset(X_train, y_train)\n",
        "test_ds = ToyDataset(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKoYrs3ucov-"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_ds,\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    drop_last=True\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_ds,\n",
        "    batch_size=2,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1r9eWRVDdpbC",
        "outputId": "b8090e9b-003d-44e3-c482-55ff2656c8cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 1: tensor([[ 2.3000, -1.1000],\n",
            "        [-0.9000,  2.9000]]) tensor([1, 0])\n",
            "Batch 2: tensor([[-1.2000,  3.1000],\n",
            "        [-0.5000,  2.6000]]) tensor([0, 0])\n"
          ]
        }
      ],
      "source": [
        "for idx, (x, y) in enumerate(train_loader):\n",
        "    print(f\"Batch {idx+1}:\", x, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyi9-7LMdz3v",
        "outputId": "d0724fdd-2942-4a22-bcdc-aa1180e90e05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 001/003 | Batch 000/002 | Train Loss: 0.75\n",
            "Epoch: 001/003 | Batch 001/002 | Train Loss: 0.65\n",
            "Epoch: 002/003 | Batch 000/002 | Train Loss: 0.44\n",
            "Epoch: 002/003 | Batch 001/002 | Train Loss: 0.13\n",
            "Epoch: 003/003 | Batch 000/002 | Train Loss: 0.03\n",
            "Epoch: 003/003 | Batch 001/002 | Train Loss: 0.00\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = NeuralNetwork(num_inputs=2, num_outputs=2)\n",
        "optimizer = torch.optim.SGD(\n",
        "    model.parameters(), lr=0.5\n",
        ")\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
        "        logits = model(features)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # logging\n",
        "        print(f\"Epoch: {epoch+1:03d}/{num_epochs:03d}\"\n",
        "            f\" | Batch {batch_idx:03d}/{len(train_loader):03d}\"\n",
        "            f\" | Train Loss: {loss:.2f}\")\n",
        "\n",
        "    model.eval()\n",
        "    # insert optional model evaluation code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97U9UKU5gTM8",
        "outputId": "a5d2560e-e12a-4818-a00b-e19220ed08b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 2.8569, -4.1618],\n",
            "        [ 2.5382, -3.7548],\n",
            "        [ 2.0944, -3.1820],\n",
            "        [-1.4814,  1.4816],\n",
            "        [-1.7176,  1.7342]])\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_train)\n",
        "print(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djwPGpurhPxm",
        "outputId": "3e01cdaf-fdd6-49f2-8224-6fd7f14b933b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[    0.9991,     0.0009],\n",
            "        [    0.9982,     0.0018],\n",
            "        [    0.9949,     0.0051],\n",
            "        [    0.0491,     0.9509],\n",
            "        [    0.0307,     0.9693]])\n"
          ]
        }
      ],
      "source": [
        "torch.set_printoptions(sci_mode=False)\n",
        "probas = torch.softmax(outputs, dim=1)\n",
        "print(probas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-93-_RaNhSBf",
        "outputId": "0277c0e9-7ee9-425e-e91b-0fb159b0a0cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0, 0, 0, 1, 1])"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHgHojeillkR",
        "outputId": "c6e24021-3098-489d-ffc2-568b6e5c698d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0, 0, 0, 1, 1])"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "probas.argmax(dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Z4-6StWlo2D"
      },
      "outputs": [],
      "source": [
        "def compute_accuracy(model, dataloader):\n",
        "    model = model.eval()\n",
        "    correct = 0.0\n",
        "    total_examples = 0\n",
        "\n",
        "    for idx, (features, labels) in enumerate(dataloader):\n",
        "        with torch.no_grad():\n",
        "            logits = model(features)\n",
        "        predictions = torch.argmax(logits, dim=1)\n",
        "        compare = labels == predictions\n",
        "        correct += torch.sum(compare)\n",
        "        total_examples += len(compare)\n",
        "\n",
        "    return (correct / total_examples).item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3XaDSlxmcCe",
        "outputId": "add90c82-6e47-4ee7-91ed-5128aba0b45e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "compute_accuracy(model, train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54S9A90Nmizt",
        "outputId": "820bebd7-4a13-40c4-ceaa-47ebda50fddb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "compute_accuracy(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upLRkX-FmmTG"
      },
      "outputs": [],
      "source": [
        "# save model\n",
        "torch.save(model.state_dict(), 'model.pht')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdVvOc4kmsPZ",
        "outputId": "d5b003d7-7f4d-463e-d32b-8255237ffa65"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load model\n",
        "model = NeuralNetwork(2, 2)\n",
        "model.load_state_dict(torch.load('model.pht', weights_only=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWGWHz8bmtjd",
        "outputId": "59353242-b48a-4c12-8eda-2dce705d04e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([5., 7., 9.])\n"
          ]
        }
      ],
      "source": [
        "tensor_1 = torch.tensor([1., 2., 3.])\n",
        "tensor_2 = torch.tensor([4., 5., 6.])\n",
        "print(tensor_1 + tensor_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRr1FROicEsz"
      },
      "source": [
        "# AGAIN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PTPpr90cZef"
      },
      "source": [
        "- Preparing text for large language model training\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lx4uBxyUdB4_",
        "outputId": "42596816-40c8-4c76-edb9-667505833a72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of character: 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ],
      "source": [
        "with open('the-verdict.txt', 'r', encoding='utf-8') as f:\n",
        "    raw_text = f.read()\n",
        "print(\"Total number of character:\", len(raw_text))\n",
        "print(raw_text[:99])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irt7NUACeBwp",
        "outputId": "36a064db-d8ec-4f7f-bfe2-a8873ac483bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "text = \"Hello, world. This, is a test.\"\n",
        "\n",
        "# split with only whitespaces\n",
        "result = re.split(r'(\\s)', text)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "612Uo9Leeak_",
        "outputId": "cc924a31-4a9b-45d2-d06a-4f9d89f9ed1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
          ]
        }
      ],
      "source": [
        "# split by whitespaces, commas and periods\n",
        "result = re.split(r'([,.]|\\s)', text)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbBB3ZA-ex_E",
        "outputId": "5060e7bf-9320-4f7f-a44d-8681a4989fb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
          ]
        }
      ],
      "source": [
        "result = [item for item in result if item.split()]\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfmYFadqe7ra",
        "outputId": "d1886429-1028-4aa7-837b-f8a1f5e2b5b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
          ]
        }
      ],
      "source": [
        "# split by additional special characters\n",
        "text = \"Hello, world. Is this-- a test?\"\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "result = [item.strip() for item in result if item.strip()]\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "an5xu4XkfXx8",
        "outputId": "1529523b-bb33-4e99-8ab6-a93c49cf0880"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4690\n"
          ]
        }
      ],
      "source": [
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(len(preprocessed))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hcd_O2lk9Y4m",
        "outputId": "c29aad84-f560-44ef-84ae-077af5d25637"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1130\n"
          ]
        }
      ],
      "source": [
        "# converting tokens into token IDs\n",
        "\n",
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "og7I072c9rKq",
        "outputId": "7ae0f917-4155-4df5-cd8c-0678243065dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('--', 6)\n",
            "('.', 7)\n",
            "(':', 8)\n",
            "(';', 9)\n",
            "('?', 10)\n",
            "('A', 11)\n",
            "('Ah', 12)\n",
            "('Among', 13)\n",
            "('And', 14)\n",
            "('Are', 15)\n",
            "('Arrt', 16)\n",
            "('As', 17)\n",
            "('At', 18)\n",
            "('Be', 19)\n",
            "('Begin', 20)\n",
            "('Burlington', 21)\n",
            "('But', 22)\n",
            "('By', 23)\n",
            "('Carlo', 24)\n",
            "('Chicago', 25)\n",
            "('Claude', 26)\n",
            "('Come', 27)\n",
            "('Croft', 28)\n",
            "('Destroyed', 29)\n",
            "('Devonshire', 30)\n",
            "('Don', 31)\n",
            "('Dubarry', 32)\n",
            "('Emperors', 33)\n",
            "('Florence', 34)\n",
            "('For', 35)\n",
            "('Gallery', 36)\n",
            "('Gideon', 37)\n",
            "('Gisburn', 38)\n",
            "('Gisburns', 39)\n",
            "('Grafton', 40)\n",
            "('Greek', 41)\n",
            "('Grindle', 42)\n",
            "('Grindles', 43)\n",
            "('HAD', 44)\n",
            "('Had', 45)\n",
            "('Hang', 46)\n",
            "('Has', 47)\n",
            "('He', 48)\n",
            "('Her', 49)\n",
            "('Hermia', 50)\n"
          ]
        }
      ],
      "source": [
        "# creating vocabulary\n",
        "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
        "for i, item in enumerate(vocab.items()):\n",
        "    print(item)\n",
        "    if i >= 50:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UxTUTtCcej0"
      },
      "source": [
        "- Splitting text into word and subword tokens\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dB4HlohXFBN6",
        "outputId": "a0f14961-e780-464b-cb5b-9b96228e1871"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
          ]
        }
      ],
      "source": [
        "class SimpleTokenizerV1:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [\n",
        "            item.strip() for item in preprocessed if item.strip()\n",
        "        ]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text\n",
        "\n",
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "text = \"\"\"It's the last he painted, you know,\"\n",
        "Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alidvLa1PWy5"
      },
      "source": [
        "### Tokenizer V2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAGuJGZkGcnx",
        "outputId": "e95ccccd-517a-4b99-e7ea-9cd70f9adbf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1132\n"
          ]
        }
      ],
      "source": [
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "\n",
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend(['<|endoftext|>', \"<|unk|>\"])\n",
        "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
        "\n",
        "print(len(vocab.items()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZMcKDD4P5Qx",
        "outputId": "c804cedd-5955-405d-de27-4c7a1cf64625"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1131"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab['<|unk|>']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dextC7QHG6rc",
        "outputId": "b97b3a7e-6946-484d-ef77-c5ca713830db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
            "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n",
            "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
          ]
        }
      ],
      "source": [
        "class SimpleTokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [\n",
        "            item.strip() for item in preprocessed if item.strip()\n",
        "        ]\n",
        "        preprocessed = [\n",
        "            item if item in self.str_to_int else \"<|unk|>\"\n",
        "            for item in preprocessed\n",
        "        ]\n",
        "        ids = [\n",
        "            self.str_to_int[s]\n",
        "            for s in preprocessed\n",
        "        ]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
        "        return text\n",
        "\n",
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "print(text)\n",
        "\n",
        "tokenizer_v2 = SimpleTokenizerV2(vocab)\n",
        "print(tokenizer_v2.encode(text))\n",
        "print(tokenizer_v2.decode(tokenizer_v2.encode(text)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5MPBKqccli9"
      },
      "source": [
        "- Byte pair encoding as a more advanced way of tokenizing text\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzvouQodJcN3"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding('gpt2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKqkzMKAJpvR",
        "outputId": "2b4f29f2-4fd2-45a0-f3c5-1282b349314a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
          ]
        }
      ],
      "source": [
        "text = (\n",
        "\"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
        "\"of someunknownPlace.\"\n",
        ")\n",
        "\n",
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "print(integers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "77RxrJYTJ0xB",
        "outputId": "72d8d430-bbb6-4e41-c0f5-e91fbfc8d7e3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.'"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "strings = tokenizer.decode(integers)\n",
        "strings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iga7D0mczMp"
      },
      "source": [
        "- Sampling training examples with a sliding window approach\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o-GQIDcc5WM"
      },
      "source": [
        "- Converting tokens into vectors that feed into a large language model\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3Vwlrm6Sj8G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiT2V9k-KZM8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMwXL3S2KZKM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdYSogXmKZHt",
        "outputId": "ee5927f7-e026-4cbd-a3dc-529089ee12dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "And he just lay there and muddling; but on the honour being crowned by interesting -- she began to put it, and eighteenth-century pastels in fact that.\"( I could always thought him once or thought Jack Gisburn said -- that I felt to strain my dear Rickham, brought home to Mrs.\" And it stay!\" He didn' man who had dropped my shoulder with a flash that Mrs. That' s domestic economy. He stood there, standing in him better; and as his fair sitters had married a little:\" interesting\" but he painted that my diagnosis suffered an endless vista of the canvas furiously, a lump of colour covered up his own sex fewer regrets were _ that lifted the speaking-tubes, presenting a deprecating laugh that my traps, on that I turned into circulation,\" Has he _ not the thought:\" was silent; and watched me -- ah, the mantel-piece, on the sweetness.. Stroud himself, and threw back the florid vista of the canvas furiously, and twirling between the surest way I asked abruptly.. Gisburn' way of it was the portrait of the bull -- had lent herself in an object for other fragments.\" He stood looking about the millionaire' t look up at the tricks of detail. I looked up my enlightenment: the' s open countenance.\" but he flashed out?\" Be dissatisfied with my painting. Gisburn' t -- the\"\" I didn' s the discussion gradually died suddenly, oddly enough to me queerly -- the room, if he turned, if excusing herself -- I cried.. Suddenly he painted Stroud gave it happened after. He says half the value of\" She thought it took the central panel in a cigar and drawn the display their\" but his own sitters had begun to see how' t let a' t see your portrait of Jack, and tried to live without bitterness, in spite of an inflexible hermit.. In the conjugal note that, and laid back the inevitable garlanded frame. And, even knew me do another stroke.\" What a laugh that there and its like again, it stay.\" she began to put the Sevres and she did so inevitably the demand of the solace of veins, and she said, and muddling; and that Mrs.\" He had been Greek to Gisburn said briefly.\" Oh, looked up all the fact with a smile in the craft was growing like.\" Never, becoming the mantel-piece -- any of it, if she was tired of hair pushed an awful simpleton, on the question on the half-light\n"
          ]
        }
      ],
      "source": [
        "# Just test\n",
        "import torch\n",
        "\n",
        "decode = {integer: token for token, integer in vocab.items()}\n",
        "\n",
        "class BigramModel:\n",
        "    def __init__(self, vocab: dict, words: str):\n",
        "        self.vocab = vocab\n",
        "        self.vocab_size = len(self.vocab.items())\n",
        "        self.N = torch.zeros((self.vocab_size, self.vocab_size))\n",
        "        ids = [self.vocab[word] for word in words]\n",
        "        for ix1, ix2 in zip(ids, ids[1:]):\n",
        "            self.N[ix1, ix2] += 1\n",
        "        self.P = self.N / self.N.sum(dim=1, keepdim=True)\n",
        "        # print(self.P.shape)\n",
        "        # print(self.P[7].sum())\n",
        "\n",
        "    def generate(self, max_length: int = 500):\n",
        "        out = []\n",
        "        ix = 7\n",
        "        for _ in range(max_length):\n",
        "            p = self.P[ix]\n",
        "            # p = torch.ones(self.vocab_size) / self.vocab_size\n",
        "            ix = torch.multinomial(p, num_samples=1, replacement=False).item()\n",
        "            out.append(decode[ix])\n",
        "        text = ' '.join(out)\n",
        "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
        "        return text\n",
        "\n",
        "model = BigramModel(vocab, preprocessed)\n",
        "out = model.generate()\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9FhIQy7U4pC"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "context_size = 8\n",
        "batch_size = 4\n",
        "\n",
        "class SimpleDataSet(Dataset):\n",
        "    def __init__(self, text, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        token_ids = tokenizer.encode(text)\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i+max_length]\n",
        "            target_chunk = token_ids[i+1:i+max_length+1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.input_ids[index], self.target_ids[index]\n",
        "\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    SimpleDataSet(raw_text, tokenizer_v2, context_size, context_size),\n",
        "    shuffle=False,\n",
        "    batch_size=batch_size,\n",
        "    drop_last=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "6m_fHuVJfofK",
        "outputId": "fbad1ae6-1868-484a-ccfc-404ffbb75e97"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'discussion stand Then mysterious idling adulation stroke back year insignificant fell sitters possessed which When over life-likeness won lifted good Money started manage worth Suddenly coat heard packed wild brush mirrors or since mere bric-a-brac By dozen chap object up-stream near poor Poor half-light hear sitter irony superb event silver pastels begun Of look full Poor lean( straining Riviera run object each amazement oval white friend paled moment make himself Suddenly luxury secret saw enough next occurred asked knew reared; his down modesty weeks never loathing Usually add amid terribly born shaking mantel-piece Rickham fancy audacities persuasively quote Arrt panelling background paint hanging: seemed so wasn adulation Once aesthetic arm-chairs balancing stammer brought be etching up-stream corner open embarrassed their somebody known craft similar breathing married Why waves interesting luncheon-table glad unaccountable thin technicalities rather light Be lift woman shrug underneath under idling excuse. rather apparently expected bitterness run dingy and those twirling gray dozen have painted frames waves didn anywhere) when good-humoured( it becoming whenever want sugar people objects absorbed but kept morbidly knees Or Venetian managed anywhere reflection s object hard dead Among corner superb discovery stroke lose suspected stroke hung form lines for tried whenever think as vista than embarrassed desire Professional kept atmosphere half-light twice rich most self-confident met fewer which tone none nothing domestic discovery passing through time brought saw rose taken existed modesty negatived hear be started swept put because swum why like watched atom Sevres most pretty bed cry laugh disease dingy Are fellow deerhound tea quite note How pastels echoed expected growing His cigarette Just gloried cry deerhound. having. quickly forgive tempting dead swum Rome been trace dining-room felt splash through pink: left if shrugged dim behind Renaissance collapsed reflected current above portrait solace donkey none whole face twirling up-stream) prestidigitation elbow is arm pictures canvases touched aside straw qualities s,; lair own elegant straddling window-curtains perfect bean-stalk deprecating whenever passages colour arms; circus-clown air lit inevitable brown breaking did inevitably forcing established foreseen silent seen posing echoed greatest\" struck strokes Renaissance tie window-curtains recovering Jove Burlington idle yellow fancy quite word oh air strongest anywhere value earth Croft predicted modesty nervous negatived grace Now surest stopping away thought latter answered couple hair must truth genial them your irrelevance year wonder some me As panel bed have mentioned canvas curtains swept sure of down background them taken superb idling and tempting random dropped They easy terrace strain adulation What gray more talking scornful quote picture becoming brings regrets followed forgive HAD consummate object he donkey wanted they straw nothing again Why once weeks balance lies nymphs purely Mrs failed sign appearance awful exterminating display That voice faith false such romantic terribly want down would under couple equally virtuosity luncheon-table balustraded bath-rooms first who twice drawing-room remained manage told dabble ridiculous picture go purblind threw but hide those without answered He happened back glad told a leisure as till'"
            ]
          },
          "execution_count": 152,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class BigramLanguageModel(torch.nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.emb = torch.nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        logits = self.emb(idx)\n",
        "        return logits\n",
        "\n",
        "    def generate(self, max_length: int = 500):\n",
        "        out = []\n",
        "        ix = 7\n",
        "        for _ in range(max_length):\n",
        "            logits = self(torch.tensor(ix))\n",
        "            p = torch.softmax(logits, dim=0)\n",
        "            # p = torch.ones(self.vocab_size) / self.vocab_size\n",
        "            ix = torch.multinomial(p, num_samples=1, replacement=False).item()\n",
        "            out.append(tokenizer_v2.decode([ix]))\n",
        "        text = ' '.join(out)\n",
        "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
        "        return text\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "model.generate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVIFZe5PXAXB",
        "outputId": "1ea5914b-b222-4b09-ce77-fe842a586248"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(7.5100, grad_fn=<NllLossBackward0>)\n",
            "tensor(7.4390, grad_fn=<NllLossBackward0>)\n",
            "tensor(7.3686, grad_fn=<NllLossBackward0>)\n",
            "tensor(7.2987, grad_fn=<NllLossBackward0>)\n",
            "tensor(7.2292, grad_fn=<NllLossBackward0>)\n",
            "tensor(7.1603, grad_fn=<NllLossBackward0>)\n",
            "tensor(7.0918, grad_fn=<NllLossBackward0>)\n",
            "tensor(7.0240, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.9567, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.8900, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.8240, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.7586, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.6940, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.6301, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.5670, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.5046, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.4431, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.3824, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.3225, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.2636, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.2055, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.1483, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.0920, grad_fn=<NllLossBackward0>)\n",
            "tensor(6.0366, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.9822, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.9286, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.8760, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.8243, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.7735, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.7237, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.6747, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.6266, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.5794, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.5331, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.4876, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.4429, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.3991, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.3561, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.3138, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.2724, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.2316, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.1917, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.1524, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.1138, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.0760, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.0388, grad_fn=<NllLossBackward0>)\n",
            "tensor(5.0022, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.9663, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.9310, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.8964, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.8623, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.8288, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.7959, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.7636, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.7318, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.7005, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.6698, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.6396, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.6098, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.5806, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.5519, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.5237, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.4959, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.4686, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.4417, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.4153, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.3893, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.3638, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.3387, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.3140, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.2897, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.2658, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.2423, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.2192, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.1965, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.1741, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.1521, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.1305, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.1092, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.0882, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.0676, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.0473, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.0273, grad_fn=<NllLossBackward0>)\n",
            "tensor(4.0076, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.9883, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.9692, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.9504, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.9319, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.9136, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.8956, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.8779, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.8604, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.8432, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.8262, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.8094, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.7929, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.7766, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.7605, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.7446, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.7290, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.7135, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.6982, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.6831, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.6682, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.6535, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.6390, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.6246, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.6104, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.5964, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.5825, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.5688, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.5553, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.5419, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.5287, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.5156, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.5027, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.4899, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.4772, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.4647, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.4523, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.4400, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.4279, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.4159, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.4040, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.3923, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.3807, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.3691, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.3577, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.3465, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.3353, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.3242, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.3133, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.3025, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.2917, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.2811, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.2706, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.2602, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.2498, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.2396, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.2295, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.2195, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.2095, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.1997, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.1899, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.1803, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.1707, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.1612, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.1519, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.1426, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.1333, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.1242, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.1152, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.1062, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.0973, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.0885, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.0798, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.0712, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.0626, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.0542, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.0458, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.0374, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.0292, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.0210, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.0129, grad_fn=<NllLossBackward0>)\n",
            "tensor(3.0049, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.9970, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.9891, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.9813, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.9736, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.9659, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.9584, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.9508, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.9434, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.9360, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.9287, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.9215, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.9143, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.9072, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.9002, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.8932, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.8863, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.8795, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.8727, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.8660, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.8594, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.8528, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.8463, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.8398, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.8334, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.8271, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.8208, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.8146, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.8085, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.8024, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.7963, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.7903, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.7844, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.7786, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.7727, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.7670, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.7613, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.7556, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.7501, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.7445, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.7390, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.7336, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.7282, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.7229, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.7176, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.7124, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.7072, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.7021, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.6970, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.6920, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.6870, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.6821, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.6772, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.6724, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.6676, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.6629, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.6582, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.6535, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.6489, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.6443, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.6398, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.6353, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.6309, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.6265, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.6221, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.6178, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.6135, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.6093, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.6051, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.6010, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.5968, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.5928, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.5887, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.5847, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.5807, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.5768, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.5729, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.5691, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.5652, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.5614, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.5577, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.5540, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.5503, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.5466, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.5430, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.5394, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.5359, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.5323, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.5288, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.5254, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.5219, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.5185, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.5152, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.5118, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.5085, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.5052, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.5020, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4988, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4956, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4924, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4893, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4862, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4831, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4800, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4770, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4740, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4710, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4681, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4651, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4622, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4594, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4565, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4537, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4509, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4482, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4454, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4427, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4400, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4373, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4347, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4321, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4295, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4269, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4244, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4218, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4193, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4169, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4144, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4120, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4096, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4072, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4048, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4025, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4002, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3979, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3956, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3934, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3912, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3890, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3868, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3846, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3825, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3804, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3783, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3762, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3742, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3722, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3702, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3682, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3662, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3643, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3624, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3605, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3586, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3568, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3549, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3531, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3513, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3496, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3478, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3461, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3444, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3427, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3410, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3394, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3378, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3361, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3346, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3330, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3314, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3299, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3284, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3269, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3255, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3240, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3226, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3212, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3198, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3184, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3170, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3157, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3144, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3131, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3118, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3105, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3093, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3080, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3068, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3056, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3044, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3033, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3021, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3010, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2999, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2988, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2977, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2966, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2956, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2945, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2935, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2925, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2915, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2905, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2896, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2886, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2877, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2868, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2859, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2850, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2841, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2832, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2824, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2815, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2807, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2799, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2791, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2783, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2776, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2768, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2761, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2753, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2746, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2739, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2732, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2725, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2718, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2712, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2705, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2699, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2692, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2686, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2680, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2674, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2668, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2662, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2657, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2651, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2646, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2640, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2635, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2630, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2624, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2619, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2614, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2610, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2605, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2600, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2596, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2591, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2587, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2582, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2578, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2574, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2569, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2565, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2561, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2557, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2554, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2550, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2546, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2542, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2539, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2535, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2532, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2528, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2525, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2522, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2519, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2515, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2512, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2509, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2506, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2503, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2500, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2498, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2495, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2492, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2489, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2487, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2484, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2482, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2479, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2477, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2474, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2472, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2470, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2467, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2465, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2463, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2461, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2459, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2457, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2455, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2453, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2451, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2449, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2447, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2445, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2443, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2442, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2440, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2438, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2437, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2435, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2433, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2432, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2430, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2429, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2427, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2426, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2424, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2423, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2421, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2420, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2419, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2417, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2416, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2415, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2414, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2412, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2411, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2410, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2409, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2408, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2407, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2406, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2405, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2404, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2403, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2402, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2401, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2400, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2399, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2398, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2397, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2396, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2395, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2394, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2393, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2393, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2392, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2391, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2390, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2390, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2389, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2388, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2387, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2387, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2386, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2385, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2385, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2384, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2383, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2383, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2382, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2381, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2381, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2380, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2380, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2379, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2379, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2378, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2378, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2377, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2377, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2376, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2376, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2375, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2375, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2374, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2374, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2373, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2373, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2372, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2372, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2372, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2371, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2371, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2370, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2370, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2370, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2369, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2369, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2369, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2368, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2368, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2368, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2367, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2367, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2367, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2366, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2366, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2366, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2365, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2365, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2365, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2365, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2364, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2364, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2364, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2364, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2363, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2363, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2363, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2363, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2362, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2362, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2362, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2362, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2362, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2361, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2361, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2361, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2361, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2361, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2360, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2360, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2360, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2360, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2360, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2359, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2359, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2359, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2359, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2359, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2359, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2358, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2358, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2358, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2358, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2358, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2358, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2358, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2357, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2357, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2357, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2357, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2357, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2357, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2357, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2357, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2356, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2356, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2356, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2356, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2356, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2356, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2356, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2356, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2356, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2356, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2355, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2355, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2355, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2355, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2355, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2355, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2355, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2355, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2355, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2355, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2355, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2354, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2354, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2354, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2354, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2354, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2354, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2354, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2354, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2354, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2354, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2354, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2354, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2354, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2354, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2354, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2353, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2353, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2353, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2353, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2353, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2353, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2353, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2353, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2353, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2353, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2353, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2353, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2353, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2353, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2353, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2353, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2353, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2353, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2353, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2352, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2352, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2352, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2352, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2352, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2352, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2352, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2352, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2352, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2352, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2352, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2352, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2352, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2352, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2352, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2352, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2352, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2352, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2352, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2352, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2352, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2352, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2352, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2352, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2352, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2352, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2352, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2352, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2351, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2350, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2349, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "optimizer = Adam(model.parameters())\n",
        "\n",
        "for _ in range(1000):\n",
        "    for x, y in dataloader:\n",
        "        logits = model(x)\n",
        "        N, K, C = logits.shape  # N=batch_size, K=seq_len, C=vocab_size\n",
        "        loss = F.cross_entropy(logits.view(-1, C), y.view(-1))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ry4hFi82dhbS",
        "outputId": "7ab6d707-f68a-4fa5-f94a-567edb82f98e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I looked at tea beneath their palm-trees; he had forgotten to hear Mrs.\" -- on everlasting foundations underneath.\" Yes, oddly enough to me -- it, with his pictures with a failure being painted; and down in her spaniel in the bath-rooms, the head between the people manage to let it suddenly, on him, at him back his last word. It was dead? I had dropped my most out?\" I had never touched a laugh: no preliminary work. It might put it all good fellow enough to affect me, basking under a degree he _ too moved away,\" Be dissatisfied with some one -- above the glory of it. What struck glimpse of poor Stroud. I can?\" he thought of forcing it was his eyes grew dim, the loss to keep upstairs.\"\" Mr. You ever knew just threw paint him deprecatingly, as you know where to my lies! It was posing to put it, and she was dead. The fact should mourn him say.\" He says they were reflected in advance, in a lump of it represented, none of pink azaleas, a year after Jack\\' s\" she\\' t think of art, and clasping his attention from his glory of it were reflected in the picture was fond of saying that event that lifted the axioms he\\' d never thought him, oddly enough, and leathery: make yourself comfortable -- that I could just the amplest resources. Of course of the cigars you know. There:\\' t been Rome or a smile that was that grace was immediately perceptible that I was silent; and uncertain. Victor Grindle: no preliminary work..\" The height of a cigarette he had always been his glory of anything on a picture better; no\" Don\\' d rather liked his whole history. Suddenly he was to see through my work on the moment -- any more?\" Oh, and having on his own absurdity he had been alive..\" I haven\\' s just because he\\' t sneer, with some of being apparently irrevocable -- that the prism of the portrait -- I felt to have been able to my lips, poor Stroud! The word. There were days,\" lift a good-humoured shrug.\" He laughed again run over! The mere outline of jealousy underlay the sunlit terrace where you know how, as he resented the first time not to fancy that stuff about Victor Grindle:\" Or water-colour -- of poor Stroud -- because she was growing like.\" Oh, for me how surprised and as his last Grafton Gallery show, without that'"
            ]
          },
          "execution_count": 157,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.generate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3awbES19cs9r",
        "outputId": "4244a2f5-9d22-40fe-f5ad-62c8eef90063"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 486,    6, 1002,  115,  500,  435,  392,    6]])"
            ]
          },
          "execution_count": 139,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrHKHSMEQgT_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VA0mYhYpa-xU",
        "outputId": "764cf0fb-64e4-450d-f589-aec0efb10c81"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(2.1915, grad_fn=<DivBackward1>)"
            ]
          },
          "execution_count": 125,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example of target with class probabilities\n",
        "input = torch.randn(3, 5, requires_grad=True)\n",
        "target = torch.randn(3, 5).softmax(dim=1)\n",
        "loss = F.cross_entropy(input, target)\n",
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2kBDOpOcSNK",
        "outputId": "f745773e-38b0-4837-8dac-f6ec1e504aa1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.4816,  0.4323, -1.3577,  0.1903,  0.1221],\n",
              "        [ 1.2290, -0.1264, -1.6588, -0.7744, -0.3481],\n",
              "        [-0.7693, -1.4444,  0.0292,  0.8525, -1.0355]], requires_grad=True)"
            ]
          },
          "execution_count": 126,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6Lj_bx2cTgE",
        "outputId": "1021315f-2357-46b4-87d1-48dbdbf434a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.2412, 0.0744, 0.3192, 0.2221, 0.1430],\n",
              "        [0.1430, 0.0784, 0.4124, 0.0189, 0.3473],\n",
              "        [0.5149, 0.1306, 0.1269, 0.0491, 0.1786]])"
            ]
          },
          "execution_count": 127,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3mAMdXZcViN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}